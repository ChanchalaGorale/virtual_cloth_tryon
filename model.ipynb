{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf### models\n",
    "import numpy as np### math computations\n",
    "import matplotlib.pyplot as plt### plotting bar chart\n",
    "import os\n",
    "import fiftyone as fo\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import albumentations as A\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from transformers import TFSegformerForSemanticSegmentation\n",
    "from transformers import create_optimizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from diffusers import StableDiffusionInpaintPipeline, EulerDiscreteScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "H,W = 512,512\n",
    "BATCH_SIZE = 2\n",
    "N_CLASSES = 24\n",
    "LR = 5e-5\n",
    "N_EPOCHS = 20\n",
    "WEIGHT_DECAY_RATE = 0.01\n",
    "MEAN = [123.675, 116.28, 103.53]\n",
    "STD = [58.395, 57.12, 57.375]\n",
    "checkpoint_filepath = \"segformer_b5_clothing.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e54fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset https://www.kaggle.com/datasets/rajkumarl/people-clothing-segmentation\n",
    "\n",
    "im_path =\"train/IMAGES/\"\n",
    "anno_path=\"train/MASKS/\"\n",
    "val_im_path=\"val/IMAGES/\"\n",
    "val_anno_path=\"val/MASKS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list=[\"0115\",\"0025\",\"0010\",\"0003\",\"0125\",\"0200\",\"0515\",\"0225\",\"0805\",\"0915\",\"0630\",\"0301\",\"0112\",\"0905\",\"0823\",\"0527\",\"0088\",\"0055\",\"0018\",\n",
    "          \"0222\",\"0049\",\"0273\",\"0299\",\"0282\",\"0372\",\"0027\",\"0445\",\"0582\",\"0374\",\"0956\",\"0211\",\"0019\",\"0961\",\"0397\",\"0699\",\"0789\",\"0996\",\"0290\",\n",
    "          \"0110\",\"0315\",\"0335\",\"0419\",\"0666\",\"0525\",\"0927\",\"0555\",\"0275\",\"0855\",\"0815\",\"0130\",\"0371\",\"0412\",\"0105\",\"0423\",\"0507\",\"0028\",\"0035\",\"0118\",\n",
    "          \"0232\",\"0849\",\"0673\",\"0688\",\"0777\",\"0472\",\"0991\",\"0485\",\"0592\",\"0334\",\"0827\",\"0651\",\"0619\",\"0567\",\"0393\",\"0609\",\"0719\",\"0916\",\"0190\",\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b05d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in val_list:\n",
    "    shutil.move(im_path+\"img_\"+name+\".png\", val_im_path+\"img_\"+name+\".png\")\n",
    "    shutil.move(anno_path+\"seg_\"+name+\".png\", val_anno_path+\"seg_\"+name+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44710c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ([im_path + i for i in os.listdir(im_path)], \n",
    "    [anno_path +\"seg\"+ i[3:] for i in os.listdir(anno_path)])\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ([val_im_path + i for i in os.listdir(val_im_path)], \n",
    "    [val_anno_path +\"seg\"+ i[3:] for i in os.listdir(val_anno_path)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b604f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [123.675, 116.28, 103.53]\n",
    "STD = [58.395, 57.12, 57.375]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(im_path, anno_path):\n",
    "  img = tf.io.decode_jpeg(tf.io.read_file(im_path))\n",
    "  img = tf.cast(img,tf.float32)\n",
    "  img = (img-MEAN)/STD\n",
    "\n",
    "  anno = tf.io.decode_jpeg(tf.io.read_file(anno_path))\n",
    "  anno = tf.cast(tf.squeeze(anno,-1),tf.float32)\n",
    "\n",
    "  return img, anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454645d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_train_ds = (\n",
    "    train_dataset\n",
    "    .map(preprocess,num_parallel_calls=tf.data.AUTOTUNE)\n",
    ")\n",
    "prep_val_ds = (\n",
    "    val_dataset\n",
    "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5882b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in prep_train_ds.take(1):\n",
    "  print(i.shape,j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "H,W = 512,512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efa649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop (H,W, p=1.0),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.Transpose(p=0.3),\n",
    "    A.Sharpen (alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.1),\n",
    "    A.RandomShadow (shadow_roi=(0, 0.5, 1, 1),\n",
    "                    num_shadows_lower=1, num_shadows_upper=2,\n",
    "                    shadow_dimension=5, p=0.1),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    #A.Resize(H,W),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(H,W),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e70d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_albument(image,mask):\n",
    "  augmented = transform(image=image, mask=mask)\n",
    "  return [tf.convert_to_tensor(augmented[\"image\"],dtype=tf.float32),\n",
    "          tf.convert_to_tensor(augmented[\"mask\"], dtype=tf.float32)]\n",
    "def val_aug_albument(image,mask):\n",
    "  augmented = val_transform(image=image, mask=mask)\n",
    "  return [tf.convert_to_tensor(augmented[\"image\"],dtype=tf.float32),\n",
    "          tf.convert_to_tensor(augmented[\"mask\"], dtype=tf.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image,mask):\n",
    "  aug_output = tf.numpy_function(func=aug_albument, inp=[image,mask], Tout=[tf.float32,tf.float32])\n",
    "  return {\"pixel_values\":tf.transpose(aug_output[0],(2,0,1)), \"labels\":aug_output[1]}\n",
    "\n",
    "def val_augment(image,mask):\n",
    "  aug_output = tf.numpy_function(func=val_aug_albument, inp=[image,mask], Tout=[tf.float32,tf.float32])\n",
    "  return {\"pixel_values\":tf.transpose(aug_output[0],(2,0,1)), \"labels\":aug_output[1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "train_ds = (\n",
    "    prep_train_ds\n",
    "    .shuffle(10)\n",
    "    .map(augment,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_ds = (\n",
    "    prep_val_ds\n",
    "    .map(val_augment,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_ds.take(1):\n",
    "  data = i\n",
    "\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (50,50))\n",
    "\n",
    "for data in train_ds.take(1):\n",
    "  images=data['pixel_values']\n",
    "  labels=data['labels']\n",
    "  for i in range(BATCH_SIZE*2):\n",
    "    if i==4:\n",
    "      break\n",
    "    ax = plt.subplot(1,BATCH_SIZE*2, i+1)\n",
    "    if i%2==0:\n",
    "      plt.imshow(tf.transpose(images[i//2], (1, 2, 0)))\n",
    "      plt.title(\"Image\")\n",
    "    else:\n",
    "      plt.imshow(labels[i//2])\n",
    "      plt.title(\"Segmentation\")\n",
    "    plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone==0.23.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = \"clothing-dataset-1\"\n",
    "data_path = \"val/IMAGES\"\n",
    "labels_path = \"val/MASKS\"\n",
    "# Create the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    data_path=data_path,\n",
    "    labels_path=labels_path,\n",
    "    dataset_type=fo.types.ImageSegmentationDirectory,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "# View summary info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/Neuralearn/data_augment --plugin-names @Neuralearn/data_augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_resized, W_resized = 512,512\n",
    "H_init, W_init = 825,550\n",
    "import evaluate\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb18b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,sample in enumerate(dataset):\n",
    "\n",
    "  img = tf.io.decode_jpeg(tf.io.read_file(sample.filepath))\n",
    "  img = tf.image.resize(img, (H_resized, W_resized))\n",
    "  img = tf.cast(img,tf.float32)\n",
    "  img = (img-MEAN)/STD\n",
    "  img = tf.transpose(img, (2,0,1))\n",
    "  img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "  output = model(img).logits\n",
    "  output = tf.argmax(output, axis=1)\n",
    "\n",
    "  resized_output = tf.image.resize(\n",
    "      tf.expand_dims(output,axis=-1),(H_init, W_init), method=\"bilinear\", antialias=True)\n",
    "  resized_output = tf.cast(tf.squeeze(resized_output, axis=-1),dtype=tf.uint8)[0]\n",
    "  sample[\"pred\"] = fo.Segmentation(mask=resized_output.numpy())\n",
    "\n",
    "  mask = cv2.imread(sample[\"ground_truth\"][\"mask_path\"], cv2.IMREAD_GRAYSCALE)\n",
    "  metrics = metric.compute(\n",
    "      predictions = [resized_output.numpy()],\n",
    "      references = [mask],\n",
    "      num_labels = len(label2id),\n",
    "      ignore_index = 0,\n",
    "      nan_to_num=0,\n",
    "      reduce_labels=False\n",
    "  )\n",
    "\n",
    "\n",
    "  sample[\"mean_iou\"] = metrics[\"mean_iou\"]\n",
    "  sample[\"mean_accuracy\"] = metrics[\"mean_accuracy\"]\n",
    "  sample[\"overall_accuracy\"] = metrics[\"overall_accuracy\"]\n",
    "\n",
    "  for k,c in enumerate(metrics[\"per_category_iou\"]):\n",
    "    if c>0.0001:\n",
    "      key = id2label[k]+\"_iou\"### e.g. shoes_iou\n",
    "      sample[key] = c\n",
    "\n",
    "  sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee51d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tf.ones([1,3,512,512])).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorShape([1, 59, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\"\n",
    "model = TFSegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = len(label2id),\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    "    ignore_mismatched_sizes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f56d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3caebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tf.zeros([1,3,H,W])).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ae2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0056b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062238db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tf.zeros([1,3,H,W])).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac61d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "512,512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  logits, labels = eval_pred\n",
    "  logits = tf.transpose(logits, perm=[0,2,3,1])\n",
    "  logits_resized =tf.image.resize(\n",
    "      logits,\n",
    "      size=tf.shape(labels)[1:],\n",
    "      method=\"bilinear\"\n",
    "  )\n",
    "  pred_labels = tf.argmax(logits_resized, axis=-1)\n",
    "\n",
    "  metrics = metric.compute(\n",
    "      predictions = pred_labels,\n",
    "      references = labels,\n",
    "      num_labels = len(label2id),\n",
    "      ignore_index = 0,\n",
    "  )\n",
    "\n",
    "  per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "  per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "  metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
    "  metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
    "\n",
    "  return {\"val_\" + k: v for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd309dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_epochs = 15\n",
    "num_train_steps = len(train_ds) * num_epochs\n",
    "learning_rate = 6e-5\n",
    "weight_decay_rate = 0.01\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce42dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=compute_metrics,\n",
    "    eval_dataset=val_ds,\n",
    "    batch_size=batch_size,\n",
    "    label_cols=[\"labels\"]\n",
    ")\n",
    "callbacks = [metric_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09dab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_resized,W_resized = 512,512\n",
    "H_init,W_init = 825,550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(im_path, mask_label):\n",
    "  mask_id = label2id[mask_label]\n",
    "  source_image = cv2.imread(im_path)\n",
    "\n",
    "  img = tf.io.decode_jpeg(tf.io.read_file(im_path))\n",
    "  img = tf.image.resize(img, (H_resized, W_resized))\n",
    "  img = tf.cast(img, tf.float32)\n",
    "  img = (img-MEAN)/STD\n",
    "  img = tf.transpose(img, (2,0,1))\n",
    "  img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "  output = model(img).logits\n",
    "\n",
    "  output = tf.argmax(output, axis=1)\n",
    "\n",
    "  resized_output = tf.image.resize(\n",
    "      tf.expand_dims(output,axis=-1),(H_init, W_init), method=\"bilinear\", antialias=True)\n",
    "  resized_output = tf.cast(tf.squeeze(resized_output, axis=-1),dtype=tf.uint8)[0].numpy()\n",
    "  mask = resized_output+(-mask_id*np.ones_like(resized_output))\n",
    "\n",
    "  mask = 255-np.clip(1e10*np.multiply(mask,mask),a_min=0, a_max=255)\n",
    "  cv2.imwrite(\"mask.jpg\", mask)\n",
    "\n",
    "  return Image.fromarray(source_image), Image.fromarray(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_inputs(\"/content/val_dataset/png_images/IMAGES/img_0003.png\",\"coat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A photorealistic photo of a woman wearing a green-colored nice looking coat all green high resolution\"\n",
    "#image and mask_image should be PIL images.\n",
    "#The mask structure is white for inpainting and black for keeping as is\n",
    "image, mask_image = generate_inputs(\n",
    "    \"/content/val_dataset/png_images/IMAGES/img_0003.png\",\"coat\")\n",
    "\n",
    "image = pipe(prompt=prompt, image=image, mask_image=mask_image, ).images[0]\n",
    "image.resize((W_init, H_init))\n",
    "display(image.resize((550,825)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2-inpainting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44975039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model_id):\n",
    "  scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "  pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id,\n",
    "                                                        scheduler=scheduler,\n",
    "                                                        revision=\"fp16\",\n",
    "                                                        torch_dtype=torch.float16)\n",
    "\n",
    "  pipe = pipe.to(\"cuda\")\n",
    "  pipe.enable_xformers_memory_efficient_attention()\n",
    "  return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb398e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = create_pipeline(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc471e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def generate_inputs(im_path,mask_path, mask_id):\n",
    "  #source_image = cv2.imread(im_path)\n",
    "  source_image = Image.open(im_path)\n",
    "  source_image = img.convert(\"RGB\")\n",
    "  sd_mask = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  out = (sd_mask+(-mask_id*np.ones_like(sd_mask)))\n",
    "  mask=-(np.clip(1e10*np.multiply(out,out),a_min=0,a_max=255)-255)\n",
    "  cv2.imwrite(\"mask.jpg\", mask)\n",
    "  pil_image = source_image.resize((512,512))\n",
    "  pil_mask = Image.fromarray(mask).resize((512,512))\n",
    "\n",
    "  return pil_image, pil_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augpaint(pipe, prompt, pil_image, pil_mask, guidance_scale, num_inference_steps):\n",
    "\n",
    "  num_images_per_prompt = 1\n",
    "  generator = torch.Generator(device=\"cuda\").manual_seed(10)\n",
    "\n",
    "  encoded_images = []\n",
    "\n",
    "  for i in range(num_images_per_prompt):\n",
    "    image = pipe(prompt=prompt, guidance_scale=guidance_scale,\n",
    "                  num_inference_steps=num_inference_steps, generator=generator,\n",
    "                  image=pil_image, mask_image=pil_mask, strength=0.99).images[0]\n",
    "\n",
    "\n",
    "\n",
    "    encoded_images.append(image.resize((550,825)))\n",
    "  return encoded_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51497e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_hash():\n",
    "  randint = random.randint(0, 100000000)\n",
    "  hash = hashlib.sha256(str(randint).encode(\"utf-8\")).hexdigest()[:10]\n",
    "  return hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e474a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_classes(mask_path):\n",
    "\n",
    "  mask = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
    "  list_unique = np.unique(mask)\n",
    "  return list_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 8\n",
    "num_inference_steps = 25\n",
    "\n",
    "\n",
    "val_im_path = \"/content/val_dataset/png_images/IMAGES/\"\n",
    "val_anno_path = \"/content/val_dataset/png_masks/MASKS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64331133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sample(sample, select_class, prompt):\n",
    "  hash = create_hash()\n",
    "  filename = sample.filepath.split(\"/\")[-1][:-4]+\"_\"+str(hash)+\".png\"\n",
    "  pipe = pipeline\n",
    "\n",
    "  im,mask = generate_inputs(\n",
    "      sample.filepath, sample.ground_truth.mask_path,\n",
    "      label2id[select_class])\n",
    "\n",
    "  out = augpaint(pipe, prompt, im, mask, guidance_scale, num_inference_steps)\n",
    "\n",
    "  #cv2.imwrite(sample.filepath[:-4]+\"_\"+str(hash)+\".png\",\n",
    "   #           np.array(out))\n",
    "  im_saved = out.save(sample.filepath[:-4]+\"_\"+str(hash)+\".png\")\n",
    "\n",
    "  shutil.copy(sample.ground_truth.mask_path,\n",
    "              sample.ground_truth.mask_path[:-4]+\"_\"+str(hash)+\".png\",\n",
    "              )\n",
    "\n",
    "  display(out)\n",
    "\n",
    "  new_sample = fo.Sample(\n",
    "    filepath=sample.filepath[:-4]+\"_\"+str(hash)+\".png\",\n",
    "    ground_truth=fo.Segmentation(\n",
    "        mask_path=sample.ground_truth.mask_path[:-4]+\"_\"+str(hash)+\".png\"),\n",
    "  )\n",
    "\n",
    "  return new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652400e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "  if sample.id == '65d62bf37ba142b66e64bcbf':\n",
    "    new_sample = transform_sample(sample, \"boots\", \"A photorealistic photo of a woman wearing a red-colored nice looking boot all red  high resolution\")\n",
    "    dataset.add_sample(new_sample)\n",
    "  #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
